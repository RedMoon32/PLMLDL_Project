{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             index  receipt_id  receipt_dayofweek receipt_time  \\\n",
       "1                1          11                  6        20:34   \n",
       "3                3          39                  4        11:28   \n",
       "4                4          39                  4        11:28   \n",
       "12              17          56                  5        11:42   \n",
       "24              40         105                  3        01:53   \n",
       "...            ...         ...                ...          ...   \n",
       "26059416  45669181     9880594                  2        20:11   \n",
       "26066451  45681543     9908635                  5        01:09   \n",
       "26071696  45690702     9929539                  0        14:39   \n",
       "26072628  45692298     9932986                  3        22:26   \n",
       "26077256  45700308     9951356                  4        03:16   \n",
       "\n",
       "                                                  item_name  item_quantity  \\\n",
       "1                                            Молоко 3,2%,шт          2.000   \n",
       "3                                 Компот из изюма, 114 ккал          1.000   \n",
       "4         Макаронные изделия отварные (масло сливочное),...          1.000   \n",
       "12                            Кофе Капучино Большой Эден 18          1.000   \n",
       "24                                   Хлеб на СЫВОРОТКЕ 350г          1.000   \n",
       "...                                                     ...            ...   \n",
       "26059416                     Напиток Энерг. Ред Булл 0,355л          1.000   \n",
       "26066451                                  Хеменгуэй Дайкири          1.000   \n",
       "26071696    Пиво светлое \"Халзан\" 4,5 % об, пл/б. 1,5 л(шт)          1.000   \n",
       "26072628                                   Экспресс педикюр          1.000   \n",
       "26077256               Конфеты Харитоша 1кг мол. ваф Яшкино          0.255   \n",
       "\n",
       "          item_price  item_nds_rate  category_id brands  \n",
       "1                  8              2           78   None  \n",
       "3                  4              1           71   None  \n",
       "4                  4              1           71   None  \n",
       "12                12              1           70   None  \n",
       "24                 7             -1           84   None  \n",
       "...              ...            ...          ...    ...  \n",
       "26059416          10              6           83   None  \n",
       "26066451          15              6            0   None  \n",
       "26071696          10              6            0   None  \n",
       "26072628          15              6           42   None  \n",
       "26077256          11              6           84   None  \n",
       "\n",
       "[48225 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>receipt_id</th>\n      <th>receipt_dayofweek</th>\n      <th>receipt_time</th>\n      <th>item_name</th>\n      <th>item_quantity</th>\n      <th>item_price</th>\n      <th>item_nds_rate</th>\n      <th>category_id</th>\n      <th>brands</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11</td>\n      <td>6</td>\n      <td>20:34</td>\n      <td>Молоко 3,2%,шт</td>\n      <td>2.000</td>\n      <td>8</td>\n      <td>2</td>\n      <td>78</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>39</td>\n      <td>4</td>\n      <td>11:28</td>\n      <td>Компот из изюма, 114 ккал</td>\n      <td>1.000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>71</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>39</td>\n      <td>4</td>\n      <td>11:28</td>\n      <td>Макаронные изделия отварные (масло сливочное),...</td>\n      <td>1.000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>71</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>17</td>\n      <td>56</td>\n      <td>5</td>\n      <td>11:42</td>\n      <td>Кофе Капучино Большой Эден 18</td>\n      <td>1.000</td>\n      <td>12</td>\n      <td>1</td>\n      <td>70</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>40</td>\n      <td>105</td>\n      <td>3</td>\n      <td>01:53</td>\n      <td>Хлеб на СЫВОРОТКЕ 350г</td>\n      <td>1.000</td>\n      <td>7</td>\n      <td>-1</td>\n      <td>84</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26059416</th>\n      <td>45669181</td>\n      <td>9880594</td>\n      <td>2</td>\n      <td>20:11</td>\n      <td>Напиток Энерг. Ред Булл 0,355л</td>\n      <td>1.000</td>\n      <td>10</td>\n      <td>6</td>\n      <td>83</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>26066451</th>\n      <td>45681543</td>\n      <td>9908635</td>\n      <td>5</td>\n      <td>01:09</td>\n      <td>Хеменгуэй Дайкири</td>\n      <td>1.000</td>\n      <td>15</td>\n      <td>6</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>26071696</th>\n      <td>45690702</td>\n      <td>9929539</td>\n      <td>0</td>\n      <td>14:39</td>\n      <td>Пиво светлое \"Халзан\" 4,5 % об, пл/б. 1,5 л(шт)</td>\n      <td>1.000</td>\n      <td>10</td>\n      <td>6</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>26072628</th>\n      <td>45692298</td>\n      <td>9932986</td>\n      <td>3</td>\n      <td>22:26</td>\n      <td>Экспресс педикюр</td>\n      <td>1.000</td>\n      <td>15</td>\n      <td>6</td>\n      <td>42</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>26077256</th>\n      <td>45700308</td>\n      <td>9951356</td>\n      <td>4</td>\n      <td>03:16</td>\n      <td>Конфеты Харитоша 1кг мол. ваф Яшкино</td>\n      <td>0.255</td>\n      <td>11</td>\n      <td>6</td>\n      <td>84</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>48225 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "train = pd.read_parquet('data_fusion_train.parquet')\n",
    "train.reset_index(inplace=True)\n",
    "train = train[train.category_id != -1].drop_duplicates('item_name')\n",
    "train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(48224, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train = train[~(train.item_name == '')]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with tensors\n",
    "import torch   \n",
    "\n",
    "#handling text data\n",
    "from torchtext import data    \n",
    "\n",
    "#Reproducing same results\n",
    "SEED = 2021\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(train, test_size=0.1, stratify=train.category_id, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(43401, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4823, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "class PandasDataFrame(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.category_id if not is_test else None\n",
    "            text = row.item_name\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train_df, val_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
    "\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data) if d is not None)\n",
    "        \n",
    "train_ds, test_ds = PandasDataFrame.splits(\n",
    "  text_field=TEXT, label_field=LABEL, train_df=train_df, val_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': ['ЧАЙ', 'ЛЕСНЫЕ', 'ЯГОДЫ', '1Л'], 'label': 83}\n{'text': ['Суп', 'Рассольник', 'Ленинградский', 'У'], 'label': 71}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "    print(vars(train_ds.examples[0])), print(vars(test_ds.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of TEXT vocabulary: 18344\nSize of LABEL vocabulary: 96\n[('с', 3832), ('1', 1480), ('в', 1320), ('для', 1037), ('шт', 957), ('и', 907), ('из', 876), ('С', 749), ('1кг', 740), ('порц.', 631)]\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_ds, min_freq=2)  \n",
    "LABEL.build_vocab(train_ds)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "#Word dictionary\n",
    "#print(TEXT.vocab.stoi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_ds, test_ds), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 40\n",
    "num_hidden_nodes = 64\n",
    "num_output_nodes = len(LABEL.vocab)\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "                   bidirectional = bidirection, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18344"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "size_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "num_output_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "classifier(\n  (embedding): Embedding(18344, 40)\n  (lstm): LSTM(40, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (fc): Linear(in_features=128, out_features=96, bias=True)\n)\nThe model has 899,744 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "if pretrained_embeddings is not None:\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-52c243e6d711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#define optimizer and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#push to cuda if available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text   \n",
    "        \n",
    "        #convert to 1D tensor\n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.label.long())        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        \n",
    "        acc = f1_score(batch.label.long().cpu().detach().numpy(), predictions.argmax(axis=1).cpu().detach().numpy(), average='weighted')   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label.long())\n",
    "\n",
    "            acc = f1_score(batch.label.long().cpu().detach().numpy(), predictions.argmax(axis=1).cpu().detach().numpy(), average='weighted') \n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tTrain Loss: 3.202 | Train Acc: 17.00%\n",
      "\t Val. Loss: 2.637 |  Val. Acc: 30.65%\n",
      "\t================\n",
      "\tTrain Loss: 2.361 | Train Acc: 38.46%\n",
      "\t Val. Loss: 2.149 |  Val. Acc: 42.18%\n",
      "\t================\n",
      "\tTrain Loss: 1.927 | Train Acc: 49.68%\n",
      "\t Val. Loss: 1.893 |  Val. Acc: 51.88%\n",
      "\t================\n",
      "\tTrain Loss: 1.638 | Train Acc: 57.35%\n",
      "\t Val. Loss: 1.726 |  Val. Acc: 55.65%\n",
      "\t================\n",
      "\tTrain Loss: 1.422 | Train Acc: 62.98%\n",
      "\t Val. Loss: 1.622 |  Val. Acc: 58.38%\n",
      "\t================\n",
      "\tTrain Loss: 1.242 | Train Acc: 67.36%\n",
      "\t Val. Loss: 1.547 |  Val. Acc: 60.92%\n",
      "\t================\n",
      "\tTrain Loss: 1.101 | Train Acc: 70.88%\n",
      "\t Val. Loss: 1.510 |  Val. Acc: 62.66%\n",
      "\t================\n",
      "\tTrain Loss: 0.980 | Train Acc: 73.86%\n",
      "\t Val. Loss: 1.510 |  Val. Acc: 63.43%\n",
      "\t================\n",
      "\tTrain Loss: 0.882 | Train Acc: 76.66%\n",
      "\t Val. Loss: 1.495 |  Val. Acc: 64.33%\n",
      "\t================\n",
      "\tTrain Loss: 0.797 | Train Acc: 78.96%\n",
      "\t Val. Loss: 1.492 |  Val. Acc: 65.24%\n",
      "\t================\n",
      "\tTrain Loss: 0.723 | Train Acc: 80.62%\n",
      "\t Val. Loss: 1.566 |  Val. Acc: 64.08%\n",
      "\t================\n",
      "\tTrain Loss: 0.670 | Train Acc: 82.18%\n",
      "\t Val. Loss: 1.541 |  Val. Acc: 65.42%\n",
      "\t================\n",
      "\tTrain Loss: 0.606 | Train Acc: 83.80%\n",
      "\t Val. Loss: 1.541 |  Val. Acc: 65.69%\n",
      "\t================\n",
      "\tTrain Loss: 0.558 | Train Acc: 85.09%\n",
      "\t Val. Loss: 1.583 |  Val. Acc: 66.11%\n",
      "\t================\n",
      "\tTrain Loss: 0.518 | Train Acc: 86.27%\n",
      "\t Val. Loss: 1.615 |  Val. Acc: 66.42%\n",
      "\t================\n",
      "\tTrain Loss: 0.482 | Train Acc: 87.12%\n",
      "\t Val. Loss: 1.620 |  Val. Acc: 66.46%\n",
      "\t================\n",
      "\tTrain Loss: 0.449 | Train Acc: 88.07%\n",
      "\t Val. Loss: 1.648 |  Val. Acc: 66.81%\n",
      "\t================\n",
      "\tTrain Loss: 0.424 | Train Acc: 88.66%\n",
      "\t Val. Loss: 1.696 |  Val. Acc: 67.14%\n",
      "\t================\n",
      "\tTrain Loss: 0.400 | Train Acc: 89.48%\n",
      "\t Val. Loss: 1.731 |  Val. Acc: 66.57%\n",
      "\t================\n",
      "\tTrain Loss: 0.379 | Train Acc: 89.91%\n",
      "\t Val. Loss: 1.762 |  Val. Acc: 66.62%\n",
      "\t================\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('\\t================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}